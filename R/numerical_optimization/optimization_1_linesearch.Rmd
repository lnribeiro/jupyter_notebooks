---
title: "Optimization -- 1 -- Line Search"
author: "Lucas N. Ribeiro"
date: "`r Sys.Date()`"
# output: pdf_document
output: html_document
---

# Introduction

I have been reading Nocedal and Wright's "Numerical Optimization". I would like to maintain a series of notebooks where I implement the numerical algorithms discussed in the book. This is a way to practice my R skills and to materialize the concepts I'm learning.

The book's first chapter is simply an introduction to optimization problems and the second chapter is an overview of of optimization algorithms:

  * Constrained vs. unconstrained optimization
  * Line search vs. trust region
  * Smooth vs. non-smooth problems
  * Scaling and rates of convergence
  
The third chapter discusses *Line search methods* and this is the subject of the present notebook.

# Line search methods

Basically, a line search method finds a direction $p_k$ and decides how far to move along this direction. These methods iterate according to

$$
  x_{k+1} = x_{k} + \alpha_k p_k
$$
where $\alpha_k \in \mathbb{R}$ is the step size.

The step direction is usually required to be a descent direction, i.e., $\nabla f_k ^T p_k < 0$. This result comes from Taylor's theorem. Assuming that $f$ is continuously differentiable, then

$$
  f(x + p) = f(x) + \nabla f(x+tp)^Tp
$$
Since we are usually interested in minimizing $f$, then $f(x+p) \leq f(x)$ and $\nabla f(x+tp)^Tp < 0$.

Search directions of interest usually have the form $p_k = -B_k^{-1} \nabla f_k$, where $B_k$ is a symmetric and non-singular matrix. In fact, we have:

$$
B_k = \begin{cases}
I                     & \text{steepest descent}\\
\nabla^2 f_k          & \text{Newton's method}\\
\approx \nabla^2 f_k  & \text{Quasi-Newton}
\end{cases}
$$

### Step length

Ideally, we would select the step size to minimize $\phi(\alpha) = f(x_k + \alpha p_k)$ for $\alpha > 0$. But *exact* line search maybe expensive sometimes. 

Usually we seek step sizes which satisfy the Wolfe conditions:

1. A step size should give sufficient decresase in the objective function $f$:

$$
  f(x_k + \alpha p_k) \leq f(x_k) + c_1 \nabla f_k^T p_k
$$
where $c_1 \in (0,1)$. 

2. curvature condition: only move to places where the slope smaller. This way, the first-order optimality condition, $\nabla f(x^*) = 0$ is seeked.

$$
  \nabla f(x_k + \alpha_k p_k)^T p_k \geq c_2 \nabla f_k^T p_k
$$
where $c_2 \in (c_1, 1)$

### Backtracking

The backtracking algorithm selects a step size with sufficient decrease. It's not optimal but it's rather simple.

1. Choose $\bar{\alpha} > 0$ and $\rho, c \in (0,1)$;
2. Set $\alpha \leftarrow \bar{\alpha}$
3. Repeat until  $f(x_k + \alpha p_k) \leq f(x_k) + c \nabla f_k^T p_k$
  * $\alpha \leftarrow \rho \alpha$
4. Terminate with $\alpha_k \leftarrow \alpha$

### Rate of convergence of line search methods

The chapter discusses convergence properties of the steepest descent, quasi-Newton and Newton's methods. The steepest descent method is globally convergent with linear convergence rate. By **globally convergent** we mean:

$$
  \lim_{k\rightarrow \infty} \| \nabla f_k \| = 0
$$
It is also proven that quasi-Newton methods have linear convergence rate and Newton's method a quadratic rate.

# Codes

Let us first implement the optimization algorithms:

```{r}

suppressPackageStartupMessages(library(tidyverse))

backtracking <- function(x, funval, fungrad, p, c = 0.5, alpha_bar = 1, rho = 0.5) {
  
  alpha <- alpha_bar
  l <- funval(x + alpha*p)
  r <- funval(x) + c*alpha*t(fungrad(x))%*%p
  
  while (l > r) {
    alpha <- rho*alpha
    l <- funval(x + alpha*p)
    r <- funval(x) + c*alpha*t(fungrad(x))%*%p
  }
  
  return(alpha)
}

gradient_descent <- function(x_init, funval, fungrad, alpha, maxit = 1000, threshold = 1e-9) {
  x <- x_init
  points <- tibble(x1 = x[1], x2 = x[2])
  
  for (it in 1:maxit) {
    # set search direction
    p <- -fungrad(x)
    
    if (alpha == "backtracking") {
      alpha <- backtracking(x = x,
                            funval = funval,
                            fungrad = fungrad,
                            p = p)
    }
    
    # move along search direction
    x <- x + alpha*p
    
    # update path dataframe
    points <- points %>% rbind(as.vector(x))
    
    # check convergence
    residual_vector <- points[it+1,] - points[it,]
    residual <- norm(as.matrix(residual_vector), type = "2")
    if (residual < threshold) break
  }
  sol <- list(x_min = x, x_points = points, n_it = it)
  return(sol)
}

newtons_method <- function(x_init, funval, fungrad, funhess, alpha = 1, maxit = 1000, threshold = 1e-9) {
  x <- x_init
  points <- tibble(x1 = x[1], x2 = x[2])
  
  for (it in 1:maxit) {
    # set Newton's direction
    p <- -solve(funhess(x)) %*% fungrad(x)
    
    # move along search direction
    x <- x + alpha*p
    
    # update path dataframe
    points <- points %>% rbind(as.vector(x))
    
    # check convergence
    residual_vector <- points[it+1,] - points[it,]
    residual <- norm(as.matrix(residual_vector), type = "2")
    if (residual < threshold) break
  }
  sol <- list(x_min = x, x_points = points, n_it = it)
  return(sol)
}

```

To experiment with the steepest descent and Newton's methods, let's consider first consider a quadratic function. It's convex and the linear searches are exact. Let 

$$
f(x) = 0.5 x^T Q x - b^Tx
$$

so its gradient is given by $\nabla f(x) = Qx - b$. Also the optimal step size is 

$$
  \alpha_k = \frac{\nabla f_k^T \nabla f_k}{\nabla f_k^T Q \nabla f_k}
$$

So, let's get to the code! First, let's define the quadratic function and its gradient.

```{r}

# Define functions and their gradients
quadr_func <- function(x, Q, b) {
  return(0.5*t(x)%*%Q%*%x - t(b)%*%x)
}

quadr_grad <- function(x, Q, b) {
  return(Q%*%x - b)
}

quadr_hess <- function(x, Q, b) {
  return(Q)
}

# Paraboloid parameters
Q = matrix(c(10, 8,
             -4, 2), 2, 2)

b = c(3, -12)

# Calculate Surface
xlims <- -200:200
ylims <- -200:200

surface <- tibble(x = xlims, y = ylims)
surface <- expand.grid(x=xlims, y=ylims) %>% 
  rowwise() %>% 
  mutate(f = quadr_func(c(x, y), Q, b))

# Perform optimization
gd_sol <- gradient_descent(x_init = c(100, 100),
                           funval = function(x) quadr_func(x, Q, b),
                           fungrad = function(x) quadr_grad(x, Q, b),
                           alpha = "backtracking"
                           # alpha = 0.1
)

nm_sol <- newtons_method(x_init = c(-100, 100),
                         funval = function(x) quadr_func(x, Q, b),
                         fungrad = function(x) quadr_grad(x, Q, b),
                         funhess = function(x) quadr_hess(x, Q, b)
)

ggplot() +
  geom_contour(data = surface, aes(x=x, y=y, z=f), bins=10) +
  geom_path(data = gd_sol$x_points, aes(x = x1, y = x2)) +
  geom_path(data = nm_sol$x_points, color = "red", aes(x = x1, y = x2)) +
  geom_point(data = gd_sol$x_points, aes(x = x1, y = x2)) +
  geom_point(data = nm_sol$x_points, color = "red", aes(x = x1, y = x2))

print("Gradient descent")
print(gd_sol$x_min)
print(quadr_func(gd_sol$x_min, Q, b))
print(gd_sol$n_it)

print("Newtons method")
print(nm_sol$x_min)
print(quadr_func(nm_sol$x_min, Q, b))
print(nm_sol$n_it)
```


Now applying to a more interesting function

```{r}
fourth_func <- function(x) {
  return(10*x[1]^4 -20*(x[1]^2)*x[2] + x[1]^2 -2*x[1] + 10*x[2]^2 + 1)
}

fourth_grad <- function(x) {
  G <- matrix(c(40*x[1]^3 - 40*x[1]*x[2] + 2*x[1] - 2,
                -20*x[1]^2 + 20*x[2]), 2, 1)
  return(G)
}

fourth_hess <- function(x) {
  H <- matrix(c(120*x[1]^2 -40*x[2]+2,
                -40*x[1],
                -40*x[1],
                20), 2, 2)
  return(H)
}

# Calculate Surface
xlims <- seq(-1000, 1000, 100)
ylims <- seq(-1000, 1000, 100)

surface <- tibble(x = xlims, y = ylims)
surface <- expand.grid(x=xlims, y=ylims) %>% 
  rowwise() %>% 
  mutate(f = quadr_func(c(x, y), Q, b))

# Perform optimization
gd_sol <- gradient_descent(x_init = c(100, 10),
                           funval = fourth_func,
                           fungrad = fourth_grad,
                           alpha = "backtracking"
)

nm_sol <- newtons_method(x_init = c(-100, 10),
                         funval = fourth_func,
                         fungrad = fourth_grad,
                         funhess = fourth_hess
)

ggplot() +
  geom_contour(data = surface, aes(x=x, y=y, z=f), bins=10) +
  geom_path(data = gd_sol$x_points, aes(x = x1, y = x2)) +
  geom_path(data = nm_sol$x_points, color = "red", aes(x = x1, y = x2)) +
  geom_point(data = gd_sol$x_points, aes(x = x1, y = x2))
  geom_point(data = nm_sol$x_points, color = "red", aes(x = x1, y = x2))

print("Gradient descent")
print(gd_sol$x_min)
print(fourth_func(gd_sol$x_min))
print(gd_sol$n_it)

print("Newtons method")
print(nm_sol$x_min)
print(fourth_func(nm_sol$x_min))
print(nm_sol$n_it)

```