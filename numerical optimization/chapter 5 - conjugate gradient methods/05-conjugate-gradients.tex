\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb, amsthm, mathtools, gensymb}
\usecolortheme{seagull}

\newcommand{\tran}{\mathsf{T}}
\newcommand{\mc}[1]{\ensuremath{\mathcal{#1}}}
\newcommand{\mbb}[1]{\ensuremath{\mathbb{#1}}}	

\title{Notes on Nocedal and Wright's ``Numerical Optimization''\\Chapter 5 --  ``Conjugate Gradient Methods''}
\author{Lucas N. Ribeiro}
\date{}
 
\begin{document}
 
\frame{\titlepage}
 
\begin{frame}[allowframebreaks]{Introduction}
	This method was proposed in the 1950s by Hestenes and Stiefel for solving large-scale linear systems of equations. Two fundamental approaches: linear and nonlinear conjugate gradient methods
	\vspace{0.5cm}
	
	Linear conjugate gradient
	\begin{itemize}
		\item Iterative method for solving $Ax = b$, where $A$ is $n\times n$ symmetric and positive definite.
		\item minimizes $\phi(x) = 0.5 x^\tran A x - b^\tran x$
	\end{itemize}
	
\end{frame}

\begin{frame}[allowframebreaks]{Conjugate Direction Methods}

	\begin{block}{Definition -- conjugacy}
		A set of nonzero vectors $\{p_0,\ldots,p_\ell\}$ is said to be \emph{conjugate} with repect to the symmetric positive definite matrix $A$ if
		\[
			p_i^\tran A p_j = 0,\quad \forall i \neq j.
		\]
	\end{block}
	
	This property lies in the fact that we can minimizes $\phi$ in $n$ steps by successively minimizing it along the individual directions in a conjugate set. Given a starting point $x_0$ and a set of conjugate directions, we generate the sequence $\{x_k\}$ by setting
	\[
		x_{k+1} = x_k + \alpha_k p_k
	\]
	where $\alpha_k = -r_k^\tran p_k/(p_k^\tran A p_k)$ is the minimizer of $\phi$ along $x_k + \alpha p_k$.
	
	\vspace{0.5cm}
	
	\begin{block}{Theorem}{Theorem 5.1}
		The sequence generated by $\{x_k\}$ considering the conjugate direction algorithm converges to the solution $x^*$ of the linear system in at most $n$ steps.
	\end{block}
	
	Interpretation: if $A$ was diagonal, then the minimizer of $\phi$ would be found by performing 1D minimizations along the coordinate directions. 
	
	Let's consider a variable transformation when $A$ is not diagonal: $\hat{x} = S^{-1}x $ where $S=[p_0,\ldots p_{n-1}]$. Then $S^\tran A S$ is diagonal (due to conjugacy) and we optimize now 
	\[
		\hat{\phi}(S\hat{x}) = 0.5 \hat{x}^\tran (S^\tran A S ) \hat{x} - (S^\tran b)^\tran \hat{x}
	\] 
	So now we can find the minimizing value of $\hat{\phi}$ by performing $n$ 1D minimizations along the coord. directions.
	
	\begin{block}{Theorem 5.2 -- Expanding subspace minimization}
	Let $x_0$ be a starting point and that $\{x_k\}$ is generated by the conjugate direction algorithm. Then $r_k^\tran p_i =0$ for $i=0,\ldots,k-1$
	\end{block} 
	This theorem establishes that the current residual $r_k$ is orthogonal to all previous search directions.
	
	Some possible conjugate vectors: eigenvectors of $A$ (but may be expensive to calculate in large problems). 

\end{frame}

\begin{frame}[allowframebreaks]{Basic Properties of the CG Method}
	Each direction $p_k$ is chosen to be a linear combination of the steepst descent direction ($-r_k$) and the previous direction $p_{k-1}$:
	\[
		p_k = -r_k + \beta_k p_{k-1}
	\]
	where $\beta_k$ is to be determined by the requirement that $p_{k-1}$ and $p_{k}$ must be conjugate w.r.t. $A$. From this, we get:
	\begin{equation}
		\beta_ k = \frac{r_k^\tran A p_{k-1}}{p_{k-1}^\tran A p_{k-1}}
	\end{equation}
	
	There is another property that establishes that the residuals $r_i$ are mutually orthogonal and that the search direction $p_k$ and residual $r_k$ are contained in the Krylov subspace of degree $k$ for $r_0$:
	\[
		r_k \in \text{span}\{r_0, Ar_0,\ldots, A^kr_0\}.
	\]
\end{frame}

\begin{frame}
	\begin{block}{CG Algorithm}
		\begin{itemize}
			\item Given $x_0$
			\item Init. $r_0 = Ax_0 - b$, $p_0 = - r_0$, $k= 0$
			\item While $r_k \neq 0$
			\begin{enumerate}
				\item Update solution: 
				\begin{itemize}
					\item $\alpha_k = r_k^\tran r_k/ (p_k^\tran A p_k)$
					\item $x_{k+1} = x_k +\alpha_k p_k$
				\end{itemize}
				\item Update residual: $r_{k+1} = r_k + \alpha_k A p_k$
				\item Calculate new conjugate direction:
				\begin{itemize}
					\item $\beta_{k+1} = r_{k+1}^\tran r_{k+1}/(r_k^\tran r_k)$
					\item $p_{k+1} = -r_{k+1} + \beta_{k+1} p_k$
				\end{itemize}
				\item $k = k+1$
			\end{enumerate}
		\end{itemize}
	\end{block}

	\begin{itemize}
		\item Depending on the eigenvalue spread of $A$, the algorithm may converge even faster than $n$ steps.
		\item Preconditioners may be applied to improve the eigenvalue distribution of $A$
	\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]{Nonlinear CG methods}

	\begin{itemize}
		
		\item So far, we considered $\phi$ as a convex quadratic function. It is possible to consider general convex (or even non-linear) functions by adapting the standard CG.
	
		\begin{itemize}
			\item Fletcher Reeves method: line search + replace residual by actual gradient
			\item Polak-Ribière method
			\item Many others...
		\end{itemize}
	
		\item Non-linear CG is appealing because each iteration requires only evaluation of the obj. function and its gradient!
	
		\item Complicated convergence analysis
		
	\end{itemize}
\end{frame}

\begin{frame}
	\begin{block}{Fletcher Reeves CG}
	\begin{itemize}
		\item Given $x_0$
		\item Init. $p_0 = -\nabla f(x_0)$, $k=0$
		\item While $\nabla f_k \neq 0$
		\begin{enumerate}
			\item Update solution: 
			\begin{itemize}
				\item Line search for $\alpha_k$
				\item $x_{k+1} = x_k +\alpha_k p_k$
			\end{itemize}
			\item Calculate new conjugate direction:
			\begin{itemize}
				\item $\beta_{k+1} = \nabla f_{k+1}^\tran \nabla f_{k+1}/(\nabla f_k^\tran \nabla f_k)$
				\item $p_{k+1} = -r_{k+1} + \beta_{k+1} p_k$
			\end{itemize}
			\item $k = k+1$
		\end{enumerate}
	\end{itemize}
\end{block}

Polak-Ribière formula: use $\beta_{k+1}^{PR} = \frac{\nabla f_{k+1}^\tran (\nabla f_{k+1} - \nabla f_{k})}{\| \nabla f_k \|^2}$

To ensure strong Wolfe's condition: $\beta_{k+1}^+ = \max \{0, \beta_{k+1}^{PR}\}$

\end{frame}
 
\end{document}

